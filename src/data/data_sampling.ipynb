{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 누락 경로 필터링 (LibriSpeech/train-other-500)\n",
    "\n",
    "#### 2. stage-1\n",
    "- task별로 분리해준 후에 \n",
    "- 오디오 길이 기준 이상치(상-하위 5%)를 제거하고\n",
    "- 다시 텍스트 기준 10분위 분할한 후에\n",
    "- 각 분위에서 설정한 갯수에 맞춰 샘플링\n",
    "\n",
    "#### 3. stage-2\n",
    "- task별로 분리해준 후에 \n",
    "- 오디오 길이 기준 이상치(상-하위 5%)를 제거하고\n",
    "- 다시 텍스트 기준 10분위 분할한 후에\n",
    "- 각 분위에서 설정한 갯수에 맞춰 샘플링\n",
    "- filtered_stage1_train.json에 대하여 asr, audiocaption task 각각에 대해\n",
    "오디오 길이 기준 상위 하위 5퍼센트를 제외하고, 중복을 피해 각 1만개씩만 stage-2로 추가 샘플링\n",
    "\n",
    "#### 4. 중복경로 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: stage1_train.json\n",
      "Before filtering: 1573558 items\n",
      "After filtering: 1424870 items\n",
      "Removed: 148688 items\n",
      "\n",
      "Saved as: filtered_stage1_train.json\n",
      "\n",
      "File: stage2_train.json\n",
      "Before filtering: 1403890 items\n",
      "After filtering: 957922 items\n",
      "Removed: 445968 items\n",
      "\n",
      "Saved as: filtered_stage2_train.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 누락경로 제거\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def filter_librispeech_other500(input_files):\n",
    "    for file_path in input_files:\n",
    "        # 파일 읽기\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 처리 전 개수 저장\n",
    "        before_count = len(data['annotation'])\n",
    "        \n",
    "        # LibriSpeech/train-other-500으로 시작하는 path를 가진 항목 필터링\n",
    "        filtered_annotations = [\n",
    "            item for item in data['annotation'] \n",
    "            if not item['path'].startswith('/LibriSpeech/train-other-500')\n",
    "        ]\n",
    "        \n",
    "        # 필터링된 결과로 데이터 업데이트\n",
    "        data['annotation'] = filtered_annotations\n",
    "        \n",
    "        # 처리 후 개수 계산\n",
    "        after_count = len(data['annotation'])\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Before filtering: {before_count} items\")\n",
    "        print(f\"After filtering: {after_count} items\")\n",
    "        print(f\"Removed: {before_count - after_count} items\\n\")\n",
    "        \n",
    "        # 현재 폴더에 새 파일명으로 저장\n",
    "        original_filename = os.path.basename(file_path)\n",
    "        new_filename = f\"filtered_{original_filename}\"\n",
    "        \n",
    "        # 한 줄로 저장 (공백 없이)\n",
    "        with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(data, ensure_ascii=False, separators=(', ', ': ')))\n",
    "        \n",
    "        print(f\"Saved as: {new_filename}\\n\")\n",
    "\n",
    "# 파일 경로 리스트\n",
    "input_files = ['stage1_train.json', 'stage2_train.json']\n",
    "\n",
    "# 실행\n",
    "filter_librispeech_other500(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /data/data_storage/GigaSpeech/26/YOU0000008873_S0000015.wav: Error opening '/data/data_storage/GigaSpeech/26/YOU0000008873_S0000015.wav': System error.\n",
      "Error processing /data/data_storage/GigaSpeech/21/YOU0000003270_S0000046.wav: Error opening '/data/data_storage/GigaSpeech/21/YOU0000003270_S0000046.wav': System error.\n",
      "Error processing /data/data_storage/GigaSpeech/26/YOU0000008877_S0000500.wav: Error opening '/data/data_storage/GigaSpeech/26/YOU0000008877_S0000500.wav': System error.\n",
      "Final dataset saved to final_stage_1.json\n"
     ]
    }
   ],
   "source": [
    "# 2. stage-1 생성\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from collections import defaultdict\n",
    "\n",
    "# 기본 설정\n",
    "JSON_DIR = \"/data/hsk/data_check\"  # JSON이 있는 디렉토리 \n",
    "FILE_DIR = \"/data/data_storage\"  # 오디오 데이터 폴더가 있는 디렉토리\n",
    "INPUT_FILE = os.path.join(JSON_DIR, \"filtered_stage1_train.json\") # 이전 단계에서 생성된 데이터 파일 이름\n",
    "OUTPUT_FILE = \"final_stage_1.json\"\n",
    "TASK_SIZES = {\"asr\": 6000, \"audiocaption\": 4000}\n",
    "\n",
    "# JSON 데이터 로드\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "data = load_json(INPUT_FILE)\n",
    "\n",
    "# Task별로 데이터 분리\n",
    "task_data = defaultdict(list)\n",
    "for item in data[\"annotation\"]:\n",
    "    task_data[item[\"task\"].lower()].append(item)\n",
    "\n",
    "# 오디오 길이 계산 함수\n",
    "def get_audio_duration(audio_path):\n",
    "    full_path = os.path.join(FILE_DIR, audio_path.lstrip(\"/\"))\n",
    "    try:\n",
    "        with sf.SoundFile(full_path) as f:\n",
    "            return len(f) / f.samplerate\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {full_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 오디오 길이 상위/하위 5% 제거\n",
    "def filter_by_duration(task_items):\n",
    "    durations = [get_audio_duration(item[\"path\"]) for item in task_items]\n",
    "    durations = np.array([d for d in durations if d is not None])\n",
    "    \n",
    "    if len(durations) == 0:\n",
    "        return []\n",
    "    \n",
    "    lower, upper = np.percentile(durations, [5, 95])\n",
    "    return [item for item, d in zip(task_items, durations) if lower <= d <= upper]\n",
    "\n",
    "# 텍스트 길이 기준 10분위 분할\n",
    "def split_by_text_length(task_items):\n",
    "    task_items.sort(key=lambda x: len(x[\"text\"]))\n",
    "    split_data = np.array_split(task_items, 10)\n",
    "    return [list(split) for split in split_data]\n",
    "\n",
    "# 각 task별 처리 및 샘플링\n",
    "final_data = []\n",
    "for task, items in task_data.items():\n",
    "    filtered_items = filter_by_duration(items)\n",
    "    split_items = split_by_text_length(filtered_items)\n",
    "    \n",
    "    if task in TASK_SIZES:\n",
    "        sample_size = TASK_SIZES[task]\n",
    "        for split in split_items:\n",
    "            final_data.extend(split[:sample_size])\n",
    "\n",
    "# 최종 JSON 저장\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"annotation\": final_data}, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Final dataset saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "# 이하 세개 오디오 파일이 정상적이지 않으므로 해당 에러는 무시\n",
    "# GigaSpeech/26/YOU0000008873_S0000015.wav\n",
    "# GigaSpeech/21/YOU0000003270_S0000046.wav\n",
    "# GigaSpeech/26/YOU0000008877_S0000500.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /data/data_storage/GigaSpeech/26/YOU0000008873_S0000015.wav: Error opening '/data/data_storage/GigaSpeech/26/YOU0000008873_S0000015.wav': System error.\n",
      "Error processing /data/data_storage/GigaSpeech/21/YOU0000003270_S0000046.wav: Error opening '/data/data_storage/GigaSpeech/21/YOU0000003270_S0000046.wav': System error.\n",
      "Error processing /data/data_storage/GigaSpeech/26/YOU0000008877_S0000500.wav: Error opening '/data/data_storage/GigaSpeech/26/YOU0000008877_S0000500.wav': System error.\n",
      "Final dataset saved to final_stage_2.json\n"
     ]
    }
   ],
   "source": [
    "# 3. stage-2 생성\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from collections import defaultdict\n",
    "\n",
    "# 기본 설정\n",
    "ROOT_DIR = \"/data/data_storage\"\n",
    "INPUT_FILES = [\n",
    "    os.path.join(ROOT_DIR, \"filtered_stage1_train.json\"),\n",
    "    os.path.join(ROOT_DIR, \"filtered_stage2_train.json\"),\n",
    "]\n",
    "OUTPUT_FILE = \"final_stage_2.json\"\n",
    "TASK_SIZES_STAGE2 = {\n",
    "    \"asr\": 3000,\n",
    "    \"audiocaption_v2\": 3200,\n",
    "    \"phone_recognition\": 2000,\n",
    "    \"qa\": 4500,\n",
    "    \"gender_recognition\": 1400,\n",
    "}\n",
    "TASK_SIZES_STAGE1 = {\"asr\": 20000, \"audiocaption\": 20000}\n",
    "\n",
    "# JSON 데이터 로드\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "data_stage1 = load_json(INPUT_FILES[0])\n",
    "data_stage2 = load_json(INPUT_FILES[1])\n",
    "\n",
    "task_data = defaultdict(list)\n",
    "for item in data_stage2[\"annotation\"]:\n",
    "    task_data[item[\"task\"].lower()].append(item)\n",
    "\n",
    "# 오디오 길이 계산 함수\n",
    "def get_audio_duration(audio_path):\n",
    "    full_path = os.path.join(ROOT_DIR, audio_path.lstrip(\"/\"))\n",
    "    try:\n",
    "        with sf.SoundFile(full_path) as f:\n",
    "            return len(f) / f.samplerate\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {full_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 오디오 길이 상위/하위 5% 제거\n",
    "def filter_by_duration(task_items):\n",
    "    durations = [get_audio_duration(item[\"path\"]) for item in task_items]\n",
    "    durations = np.array([d for d in durations if d is not None])\n",
    "    \n",
    "    if len(durations) == 0:\n",
    "        return []\n",
    "    \n",
    "    lower, upper = np.percentile(durations, [5, 95])\n",
    "    return [item for item, d in zip(task_items, durations) if lower <= d <= upper]\n",
    "\n",
    "# 텍스트 길이 기준 10분위 분할\n",
    "def split_by_text_length(task_items):\n",
    "    task_items.sort(key=lambda x: len(x[\"text\"]))\n",
    "    split_data = np.array_split(task_items, 10)\n",
    "    return [list(split) for split in split_data]\n",
    "\n",
    "# 각 task별 처리 및 샘플링\n",
    "final_data = []\n",
    "for task, items in task_data.items():\n",
    "    filtered_items = filter_by_duration(items)\n",
    "    split_items = split_by_text_length(filtered_items)\n",
    "    \n",
    "    if task in TASK_SIZES_STAGE2:\n",
    "        sample_size = TASK_SIZES_STAGE2[task]\n",
    "        for split in split_items:\n",
    "            final_data.extend(split[:sample_size])\n",
    "\n",
    "# Stage1에서 asr, audiocaption 추가\n",
    "task_data_stage1 = defaultdict(list)\n",
    "for item in data_stage1[\"annotation\"]:\n",
    "    if item[\"task\"].lower() in TASK_SIZES_STAGE1:\n",
    "        task_data_stage1[item[\"task\"].lower()].append(item)\n",
    "\n",
    "for task, items in task_data_stage1.items():\n",
    "    filtered_items = filter_by_duration(items)\n",
    "    selected_items = filtered_items[:TASK_SIZES_STAGE1[task]]\n",
    "    final_data.extend(selected_items)\n",
    "\n",
    "# 최종 JSON 저장\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"annotation\": final_data}, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Final dataset saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "# 이하 세개 오디오 파일이 정상적이지 않으므로 해당 에러는 무시\n",
    "# GigaSpeech/26/YOU0000008873_S0000015.wav\n",
    "# GigaSpeech/21/YOU0000003270_S0000046.wav\n",
    "# GigaSpeech/26/YOU0000008877_S0000500.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "중복 제거 전 task 별 개수:\n",
      "asr: 60000\n",
      "audiocaption: 40000\n",
      "총 개수: 100000\n",
      "\n",
      "중복 제거 후 task 별 개수:\n",
      "asr: 60000\n",
      "audiocaption: 39731\n",
      "총 개수: 99731\n",
      "\n",
      "제거된 중복 항목 수: 269\n",
      "\n",
      "중복이 제거된 데이터가 저장됨: final_stage_1_no_duplicates.json\n"
     ]
    }
   ],
   "source": [
    "# 4-1 stage-1 중복 제거\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def process_data(file_path):\n",
    "    # 파일 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 중복 제거 전 task 개수 계산\n",
    "    before_counts = Counter(item['task'] for item in data['annotation'])\n",
    "    print(\"\\n중복 제거 전 task 별 개수:\")\n",
    "    for task, count in before_counts.items():\n",
    "        print(f\"{task}: {count}\")\n",
    "    print(f\"총 개수: {len(data['annotation'])}\")\n",
    "    \n",
    "    # path를 기준으로 중복 제거\n",
    "    seen_paths = set()\n",
    "    unique_annotations = []\n",
    "    \n",
    "    for item in data['annotation']:\n",
    "        if item['path'] not in seen_paths:\n",
    "            seen_paths.add(item['path'])\n",
    "            unique_annotations.append(item)\n",
    "    \n",
    "    # 중복 제거 후 task 개수 계산\n",
    "    after_counts = Counter(item['task'] for item in unique_annotations)\n",
    "    print(\"\\n중복 제거 후 task 별 개수:\")\n",
    "    for task, count in after_counts.items():\n",
    "        print(f\"{task}: {count}\")\n",
    "    print(f\"총 개수: {len(unique_annotations)}\")\n",
    "    \n",
    "    # 제거된 중복 개수 계산\n",
    "    duplicates_removed = len(data['annotation']) - len(unique_annotations)\n",
    "    print(f\"\\n제거된 중복 항목 수: {duplicates_removed}\")\n",
    "    \n",
    "    # 결과를 새 파일로 저장\n",
    "    output_data = {'annotation': unique_annotations}\n",
    "    output_path = file_path.replace('.json', '_no_duplicates.json')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\n중복이 제거된 데이터가 저장됨: {output_path}\")\n",
    "\n",
    "# 파일 처리 실행\n",
    "file_path = 'final_stage_1.json'\n",
    "process_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "중복 제거 전 task 별 개수:\n",
      "asr: 50000\n",
      "QA: 45000\n",
      "phone_recognition: 20000\n",
      "audiocaption_v2: 32000\n",
      "gender_recognition: 14000\n",
      "audiocaption: 20000\n",
      "총 개수: 181000\n",
      "\n",
      "중복 제거 후 task 별 개수:\n",
      "asr: 37098\n",
      "QA: 43762\n",
      "phone_recognition: 15871\n",
      "audiocaption_v2: 26979\n",
      "gender_recognition: 9435\n",
      "audiocaption: 17444\n",
      "총 개수: 150589\n",
      "\n",
      "제거된 중복 항목 수: 30411\n",
      "\n",
      "중복이 제거된 데이터가 저장됨: final_stage_2_no_duplicates.json\n"
     ]
    }
   ],
   "source": [
    "# 4-2 stage-2 중복 제거\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def process_data(file_path):\n",
    "    # 파일 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 중복 제거 전 task 개수 계산\n",
    "    before_counts = Counter(item['task'] for item in data['annotation'])\n",
    "    print(\"\\n중복 제거 전 task 별 개수:\")\n",
    "    for task, count in before_counts.items():\n",
    "        print(f\"{task}: {count}\")\n",
    "    print(f\"총 개수: {len(data['annotation'])}\")\n",
    "    \n",
    "    # path를 기준으로 중복 제거\n",
    "    seen_paths = set()\n",
    "    unique_annotations = []\n",
    "    \n",
    "    for item in data['annotation']:\n",
    "        if item['path'] not in seen_paths:\n",
    "            seen_paths.add(item['path'])\n",
    "            unique_annotations.append(item)\n",
    "    \n",
    "    # 중복 제거 후 task 개수 계산\n",
    "    after_counts = Counter(item['task'] for item in unique_annotations)\n",
    "    print(\"\\n중복 제거 후 task 별 개수:\")\n",
    "    for task, count in after_counts.items():\n",
    "        print(f\"{task}: {count}\")\n",
    "    print(f\"총 개수: {len(unique_annotations)}\")\n",
    "    \n",
    "    # 제거된 중복 개수 계산\n",
    "    duplicates_removed = len(data['annotation']) - len(unique_annotations)\n",
    "    print(f\"\\n제거된 중복 항목 수: {duplicates_removed}\")\n",
    "    \n",
    "    # 결과를 새 파일로 저장\n",
    "    output_data = {'annotation': unique_annotations}\n",
    "    output_path = file_path.replace('.json', '_no_duplicates.json')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\n중복이 제거된 데이터가 저장됨: {output_path}\")\n",
    "\n",
    "# 파일 처리 실행\n",
    "file_path = 'final_stage_2.json'\n",
    "process_data(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
